# ç‰¹å¾åˆ†ç±»
import os
import cv2

from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
import numpy as np
from utils.Net import FeatureNet
import torch
import uuid

Modespaht=r"I:\python-Code\Supermarketsettlement\scripts\runs\models\bag\best.pt"
model=FeatureNet()
model.load_state_dict(torch.load(r'I:\python-Code\Supermarketsettlement\scripts\runs\models\bag\best.pt',
                      map_location=torch.device('cuda'))
                      )

# æ¨èï¼šå°†æ¨¡å‹åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropoutã€batch norm ç­‰è®­ç»ƒç›¸å…³è¡Œä¸ºï¼‰
model.eval()



def DATA(img):
    # 3. é¢„å¤„ç†å›¾åƒï¼ˆå…³é”®ï¼å¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB
    img = cv2.resize(img, (224, 224))  # è°ƒæ•´å¤§å°ï¼ˆæ ¹æ®ä½ æ¨¡å‹è¾“å…¥ï¼‰
    img = img.astype(np.float32) / 255.0  # å½’ä¸€åŒ–åˆ° [0,1]
    img = np.transpose(img, (2, 0, 1))  # HWC â†’ CHW
    input_tensor = torch.from_numpy(img).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦: (1, C, H, W)
    return input_tensor









# 1. è¿æ¥æœ¬åœ° Qdrant
client = QdrantClient(host="localhost", port=6333)
print("âœ… æˆåŠŸè¿æ¥åˆ° Qdrant")

# 2. å®šä¹‰ collection åç§°
collection_name = "imagesTTTxindex"

# 3. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨å°±ä¸é‡å¤åˆ›å»º
if not client.collection_exists(collection_name):
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(
            size=512,
            distance=Distance.COSINE
        )
    )
    print(f"âœ… å·²åˆ›å»º collection '{collection_name}'")
else:
    print(f"â„¹ï¸  collection '{collection_name}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")

# path=r"I:\python-Code\Supermarketsettlement\DATA\dataset\bag"
# img_list=os.listdir(path)
#
# for index, imgname in enumerate(img_list):
#     img=os.path.join(path,imgname)
#     img = cv2.imread(img)
#     input_tensor=DATA(img)
#     with torch.no_grad():
#      output = model(input_tensor)
#
#     client.upsert(
#         collection_name=collection_name,
#         points=[{"id":index, "vector": output[0].tolist(),
#                  "payload": {"name": imgname}}]
#     )
#     print("æ’å…¥æˆå…±")
#


# 5. æ’å…¥æ•°æ®


input_tensor=DATA(cv2.imread(r"I:\python-Code\DATA\good_good_data\good_good_data\bag\train\4\13_aug4.jpg"))
with torch.no_grad():
    output = model(input_tensor)



# 6. æœç´¢æœ€ç›¸ä¼¼çš„å›¾åƒï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºæŸ¥è¯¢ï¼‰
query_vector = output[0].tolist()

# âœ… ä½¿ç”¨ query_points å¹¶æ­£ç¡®è·å– .points
results = client.query_points(
    collection_name=collection_name,
    query=query_vector,  # â† å‚æ•°åæ˜¯ query
    limit=100,
).points  # â† å¿…é¡»åŠ  .points æ‰èƒ½æ‹¿åˆ°ç»“æœåˆ—è¡¨

print("\nğŸ” æœ€ç›¸ä¼¼çš„å›¾åƒï¼š")
for hit in results:
    print(f"ID: {hit.id}, ç›¸ä¼¼åº¦å¾—åˆ†: {hit.score:.4f}ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šç›¸ä¼¼ï¼‰name:{hit.payload}")